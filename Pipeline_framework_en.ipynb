{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline Framework \n",
    "This is a notebook for illustrating the pipeline framework of our project. Our project can be divided into 5 steps:\n",
    "1. Split text and candidate summary into two lists of sentences.\n",
    "2. Convert those lists of sentences to embedding matrix.\n",
    "3. Calculate the cosine similarity between sentences of summary and sentences of text based on their embeddings.\n",
    "4. Find the indices of top k related sentences in text for each sentence in summary.\n",
    "5. Check if the sentence from the summary can be obtained from the sentence from the text with the help of LLMs.\n",
    "\n",
    "The pipeline framework is just a toy model. There might be some possible improvements. For example, we can try to check if the dependency arcs or name entities in the summary sentence can be obtained from the related sentences in the original text with the help of LLMs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3f6f72c386d1239"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to import some packages and initialize some tools in advance. \n",
    "1. `nlp` is a tool for splitting text into sentences.\n",
    "2. `model` is a tool for converting sentences to embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774419141c587d3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = stanza.Pipeline(lang='en')   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619c6425d0258220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step one: Split text and candidate summary into two lists of sentences.\n",
    "We use `nlp` to split text and summaries into sentences. This will help us to check if the sentence from the summary can be obtained from specific sentences from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12099d842d305882"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_text(text:str)->list:\n",
    "    \"\"\"\n",
    "    Split text into sentences\n",
    "    Args:\n",
    "        text: the text to be split\n",
    "\n",
    "    Returns:\n",
    "        a list of sentences\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sentence.text for sentence in doc.sentences]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c7ef9a37d15a703"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step two: Convert those lists of sentences to embedding matrix.\n",
    "We use `model` to convert sentences to embeddings. The output is a matrix with the type of `np.ndarray`, each row is an embedding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975e184d6fe7ed7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence2embedding(sentences:list[str])->np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert sentences to embeddings\n",
    "    Args:\n",
    "        sentences: a list of sentences\n",
    "\n",
    "    Returns:\n",
    "        a matrix of embeddings, each row is an embedding\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0c16bad9c304d7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step three: Get the most related sentences from the original text for each sentence in the summary.\n",
    "- We use cosine similarity to calculate the similarity between sentences of summary and sentences of text. The output is a matrix with the type of `np.ndarray`.\n",
    "- Assume there are $M$ sentences in the original text and $N$ sentences in the summary, the output matrix is of shape $N\\times M$. \n",
    "- The `[i,j]` element of the matrix is the cosine similarity between the $i$-th sentence in the summary and the $j$-th sentence in the original text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aaaeb9894375270"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcosine_similarity\u001B[39m(embed_text:np\u001B[38;5;241m.\u001B[39mndarray, embed_summary: np\u001B[38;5;241m.\u001B[39mndarray)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39mnp\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Calculate the cosine similarities between sentences of summary and sentences of text\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m        a matrix of cosine similarities\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     dot_prod \u001B[38;5;241m=\u001B[39m embed_summary \u001B[38;5;241m@\u001B[39m embed_text\u001B[38;5;241m.\u001B[39mT \u001B[38;5;66;03m# [i,j] is the dot product of summary sentence i and text sentence j\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(embed_text:np.ndarray, embed_summary: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarities between sentences of summary and sentences of text\n",
    "    Args:\n",
    "        embed_text: embedding matrix of text sentences\n",
    "                    each row is an embedding\n",
    "        embed_summary: embedding matrix of summary sentences\n",
    "                    each row is an embedding\n",
    "\n",
    "    Returns:\n",
    "        a matrix of cosine similarities\n",
    "    \"\"\"\n",
    "    \n",
    "    dot_prod = embed_summary @ embed_text.T # [i,j] is the dot product of summary sentence i and text sentence j\n",
    "    norm = np.linalg.norm(embed_summary, axis=1) @ np.linalg.norm(embed_text, axis=1).T # [i,j] is the norm of summary sentence i and text sentence j\n",
    "    return dot_prod / norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:50:50.721424Z",
     "start_time": "2023-11-02T00:50:50.514851Z"
    }
   },
   "id": "8a0e16c7add78146"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we will find the indices of top k related sentences in text for each sentence in summary. Those selected sentences from the original text will be used in the prompt of LLMs for checking if the sentence from the summary can be obtained from the sentence from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4150dfa4365de0da"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtopk_related\u001B[39m(sim_matrix:np\u001B[38;5;241m.\u001B[39mndarray, k:\u001B[38;5;28mint\u001B[39m)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39mnp\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Find the indices of top k related sentences in text for each sentence in summary\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m        a matrix of indices\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sim_matrix\u001B[38;5;241m.\u001B[39margsort(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[:, \u001B[38;5;241m-\u001B[39mk:]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def topk_related(sim_matrix:np.ndarray, k:int)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the indices of top k related sentences in text for each sentence in summary\n",
    "    Args:\n",
    "        sim_matrix: cosine similarity matrix\n",
    "        k: number of sentences to be selected\n",
    "\n",
    "    Returns:\n",
    "        a matrix of indices\n",
    "    \"\"\"\n",
    "    return sim_matrix.argsort(axis=1)[:, -k:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:50:51.156315Z",
     "start_time": "2023-11-02T00:50:51.146870Z"
    }
   },
   "id": "d8b0483a981dfd9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step four: Check if the sentence from the summary can be obtained from the sentence from the text with the help of LLMs.\n",
    "For each sentence in the summary, check if it can be obtained from the top k related sentences in the text.\n",
    "1. If yes, return True\n",
    "2. Otherwise, return False.\n",
    "\n",
    "Meanwhile, we can also return the probability that the sentence from the summary can be obtained from the sentence from the text.\n",
    "\n",
    "We just consider the factuality in sentence-level currently.\n",
    "\n",
    "This part will employ LLMs and [Guidance](https://github.com/guidance-ai/guidance) to check if the sentence from the summary can be obtained from the sentence from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c7e34f086508c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def checker(sens_text:list[str], sen_summary:str)->(bool, float):\n",
    "    \"\"\"\n",
    "    Check if the sentence from the summary con be obtained from the sentence from the text.\n",
    "    Args:\n",
    "        sens_text: list of sentences from the text\n",
    "        sen_summary: the sentence from the summary\n",
    "\n",
    "    Returns:\n",
    "        a tuple of (bool, float)\n",
    "        bool: True if the sentence from the summary can be obtained from the sentence from the text\n",
    "        float: the probability that the sentence from the summary can be obtained from the sentence from the text\n",
    "            True: >0.5\n",
    "            False: <0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    # to be completed\n",
    "    \n",
    "    \n",
    "    \n",
    "    res = ____\n",
    "    prob = ____\n",
    "    \n",
    "    return (res, prob)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9bd2cba2beadd85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step five: Evaluate the quality of the summary (Combine the above steps).\n",
    "We combine the above steps to evaluate the quality of the summary. \n",
    "\n",
    "We will get a score between 0 and 1, the higher the better."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46f93591cc12a7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(text:str, summary:str, k:int)->float:\n",
    "    \"\"\"\n",
    "    evaluate the quality of the summary according to the given text\n",
    "    Args:\n",
    "        text: original text\n",
    "        summary: summary to be evaluated\n",
    "        k: number of sentences to be selected from the text\n",
    "\n",
    "    Returns:\n",
    "        a float number between 0 and 1, the higher the better\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the text into sentences\n",
    "    sens_text = split_text(text)\n",
    "    # split the summary into sentences\n",
    "    sens_summary = split_text(summary)\n",
    "    \n",
    "    # convert sentences to embeddings\n",
    "    embed_text = sentence2embedding(sens_text)\n",
    "    embed_summary = sentence2embedding(sens_summary)\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    sim_matrix = cosine_similarity(embed_text, embed_summary)\n",
    "    \n",
    "    # find top k related sentences\n",
    "    topk = topk_related(sim_matrix, k)\n",
    "    \n",
    "    # check if the sentence from the summary can be obtained from the sentence from the text\n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    for idx, sen in enumerate(sens_summary):\n",
    "        sens_text_selected = [sens_text[i] for i in topk[idx]]\n",
    "        res, _ = checker(sens_text_selected, sen)\n",
    "        if res:\n",
    "            numerator += 1\n",
    "        denominator += 1\n",
    "    return numerator / denominator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c33101f07b1a506d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
