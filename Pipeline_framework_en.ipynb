{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline Framework \n",
    "This is a notebook for illustrating the pipeline framework of our project. Our project can be divided into 5 steps:\n",
    "1. Split text and candidate summary into two lists of sentences.\n",
    "2. Convert those lists of sentences to embedding matrix.\n",
    "3. Calculate the cosine similarity between sentences of summary and sentences of text based on their embeddings.\n",
    "4. Find the indices of top k related sentences in text for each sentence in summary.\n",
    "5. Check if the sentence from the summary can be obtained from the sentence from the text with the help of LLMs.\n",
    "\n",
    "The pipeline framework is just a toy model. There might be some possible improvements. For example, we can try to check if the dependency arcs or name entities in the summary sentence can be obtained from the related sentences in the original text with the help of LLMs. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3f6f72c386d1239"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:06:11.376548Z",
     "start_time": "2023-11-02T02:06:06.572675Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to import some packages and initialize some tools in advance. \n",
    "1. `nlp` is a tool for splitting text into sentences.\n",
    "2. `model` is a tool for converting sentences to embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774419141c587d3e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)821d1/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15f334a462554283ab3ab421dddc6cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b148c722653d4d159b33cbcb6ee78001"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)8d01e821d1/README.md:   0%|          | 0.00/3.95k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3093ad33af1e4cb1a222b04afbbd3042"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)d1/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91b7464c7754496d890b9333c893b99b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)01e821d1/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09907206c0d34567b37d8bda62b67e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22662bae3a294d8ea8838763aa7025fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2653e57bf854223bed290fd28e5b3a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16fbe157d661421b810a8ef088b230e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bcbd15477bf4e399a8c6e0ab276e159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)821d1/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eec6aa62cfca4490bf23fa444c4fb6e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "056461cab1884ba18ccea507e91e292f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)8d01e821d1/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a8aff7a5ea4e898d7845dec08e6502"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)1e821d1/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52cdf1513c254f31adb8fc6f891cfb35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 22:06:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ef09950c7d446d3b4b3477b111fb6e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/pos/combined_charlm.pt:   0%|  …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdf7ebd682334b64b871c375d86813ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 22:06:53 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-01 22:06:53 INFO: Using device: cpu\n",
      "2023-11-01 22:06:53 INFO: Loading: tokenize\n",
      "2023-11-01 22:06:53 INFO: Loading: pos\n",
      "2023-11-01 22:06:53 INFO: Loading: lemma\n",
      "2023-11-01 22:06:53 INFO: Loading: constituency\n",
      "2023-11-01 22:06:54 INFO: Loading: depparse\n",
      "2023-11-01 22:06:54 INFO: Loading: sentiment\n",
      "2023-11-01 22:06:54 INFO: Loading: ner\n",
      "2023-11-01 22:06:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "nlp = stanza.Pipeline(lang='en')   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:06:55.114465Z",
     "start_time": "2023-11-02T02:06:13.372718Z"
    }
   },
   "id": "619c6425d0258220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step one: Split text and candidate summary into two lists of sentences.\n",
    "We use `nlp` to split text and summaries into sentences. This will help us to check if the sentence from the summary can be obtained from specific sentences from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12099d842d305882"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def split_text(text:str)->list:\n",
    "    \"\"\"\n",
    "    Split text into sentences\n",
    "    Args:\n",
    "        text: the text to be split\n",
    "\n",
    "    Returns:\n",
    "        a list of sentences\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sentence.text for sentence in doc.sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:02.336313Z",
     "start_time": "2023-11-02T02:07:02.331361Z"
    }
   },
   "id": "8c7ef9a37d15a703"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step two: Convert those lists of sentences to embedding matrix.\n",
    "We use `model` to convert sentences to embeddings. The output is a matrix with the type of `np.ndarray`, each row is an embedding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975e184d6fe7ed7e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def sentence2embedding(sentences:list[str])->np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert sentences to embeddings\n",
    "    Args:\n",
    "        sentences: a list of sentences\n",
    "\n",
    "    Returns:\n",
    "        a matrix of embeddings, each row is an embedding\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:03.181822Z",
     "start_time": "2023-11-02T02:07:03.165022Z"
    }
   },
   "id": "d0c16bad9c304d7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step three: Get the most related sentences from the original text for each sentence in the summary.\n",
    "- We use cosine similarity to calculate the similarity between sentences of summary and sentences of text. The output is a matrix with the type of `np.ndarray`.\n",
    "- Assume there are $M$ sentences in the original text and $N$ sentences in the summary, the output matrix is of shape $N\\times M$. \n",
    "- The `[i,j]` element of the matrix is the cosine similarity between the $i$-th sentence in the summary and the $j$-th sentence in the original text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aaaeb9894375270"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def cosine_similarity(embed_text:np.ndarray, embed_summary: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarities between sentences of summary and sentences of text\n",
    "    Args:\n",
    "        embed_text: embedding matrix of text sentences\n",
    "                    each row is an embedding\n",
    "        embed_summary: embedding matrix of summary sentences\n",
    "                    each row is an embedding\n",
    "\n",
    "    Returns:\n",
    "        a matrix of cosine similarities\n",
    "    \"\"\"\n",
    "    \n",
    "    dot_prod = embed_summary @ embed_text.T # [i,j] is the dot product of summary sentence i and text sentence j\n",
    "    norm = np.linalg.norm(embed_summary, axis=1, keepdims=True) @ np.linalg.norm(embed_text, axis=1, keepdims=True).T # [i,j] is the norm of summary sentence i and text sentence j\n",
    "    return dot_prod / norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:03.846945Z",
     "start_time": "2023-11-02T02:07:03.839862Z"
    }
   },
   "id": "8a0e16c7add78146"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we will find the indices of top k related sentences in text for each sentence in summary. Those selected sentences from the original text will be used in the prompt of LLMs for checking if the sentence from the summary can be obtained from the sentence from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4150dfa4365de0da"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def topk_related(sim_matrix:np.ndarray, k:int)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the indices of top k related sentences in text for each sentence in summary\n",
    "    Args:\n",
    "        sim_matrix: cosine similarity matrix\n",
    "        k: number of sentences to be selected\n",
    "\n",
    "    Returns:\n",
    "        a matrix of indices\n",
    "    \"\"\"\n",
    "    return sim_matrix.argsort(axis=1)[:, -k:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:04.499175Z",
     "start_time": "2023-11-02T02:07:04.492510Z"
    }
   },
   "id": "d8b0483a981dfd9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step four: Check if the sentence from the summary can be obtained from the sentence from the text with the help of LLMs.\n",
    "For each sentence in the summary, check if it can be obtained from the top k related sentences in the text.\n",
    "1. If yes, return True\n",
    "2. Otherwise, return False.\n",
    "\n",
    "Meanwhile, we can also return the probability that the sentence from the summary can be obtained from the sentence from the text.\n",
    "\n",
    "We just consider the factuality in sentence-level currently.\n",
    "\n",
    "This part will employ LLMs and [Guidance](https://github.com/guidance-ai/guidance) to check if the sentence from the summary can be obtained from the sentence from the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d2c7e34f086508c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def checker(sens_text:list[str], sen_summary:str)->(bool, float):\n",
    "    \"\"\"\n",
    "    Check if the sentence from the summary con be obtained from the sentence from the text.\n",
    "    Args:\n",
    "        sens_text: list of sentences from the text\n",
    "        sen_summary: the sentence from the summary\n",
    "\n",
    "    Returns:\n",
    "        a tuple of (bool, float)\n",
    "        bool: True if the sentence from the summary can be obtained from the sentence from the text\n",
    "        float: the probability that the sentence from the summary can be obtained from the sentence from the text\n",
    "            True: >0.5\n",
    "            False: <0.5\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    source_text = ''.join(sens_text)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "As a compliance officer at a financial institution, you're tasked with evaluating the accuracy of a summary sentence based on its alignment with source sentences from a financial document. Consider the following criteria carefully:\n",
    "\n",
    "1. The summary accurately reflects the content of the source sentences, especially numerical information.\n",
    "2. All named entities in the summary are present in the source sentences.\n",
    "3. Relationships between entities in the summary are consistent with those in the source sentences.\n",
    "4. The directional flow of relationships among named entities matches between the summary and source sentences.\n",
    "5. There are no factual discrepancies between the summary and source sentences.\n",
    "6. The summary does not introduce any entities not found in the source sentences.\n",
    "\n",
    "Your job is to determine if the summary adheres to these criteria. Answer \"Yes\" if it does, or \"No\" if it doesn't.\n",
    "\n",
    "Summary sentence: ```{sen_summary}```\n",
    "\n",
    "Source sentences: ```{source_text}```\n",
    "\n",
    "Final Answer (Yes/No only): \n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4',\n",
    "        messages=[{'role':\"user\",'content':prompt}],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    \n",
    "    res = response.choices[0].text.lower().capitalize()\n",
    "    \n",
    "    return eval(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:04.992541Z",
     "start_time": "2023-11-02T02:07:04.987484Z"
    }
   },
   "id": "f9bd2cba2beadd85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step five: Evaluate the quality of the summary (Combine the above steps).\n",
    "We combine the above steps to evaluate the quality of the summary. \n",
    "\n",
    "We will get a score between 0 and 1, the higher the better."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46f93591cc12a7e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def evaluate(text:str, summary:str, k:int)->float:\n",
    "    \"\"\"\n",
    "    evaluate the quality of the summary according to the given text\n",
    "    Args:\n",
    "        text: original text\n",
    "        summary: summary to be evaluated\n",
    "        k: number of sentences to be selected from the text\n",
    "\n",
    "    Returns:\n",
    "        a float number between 0 and 1, the higher the better\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the text into sentences\n",
    "    sens_text = split_text(text)\n",
    "    # split the summary into sentences\n",
    "    sens_summary = split_text(summary)\n",
    "    \n",
    "    # convert sentences to embeddings\n",
    "    embed_text = sentence2embedding(sens_text)\n",
    "    embed_summary = sentence2embedding(sens_summary)\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    sim_matrix = cosine_similarity(embed_text, embed_summary)\n",
    "    \n",
    "    # find top k related sentences\n",
    "    topk = topk_related(sim_matrix, k)\n",
    "    \n",
    "    # check if the sentence from the summary can be obtained from the sentence from the text\n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    for idx, sen in enumerate(sens_summary):\n",
    "        sens_text_selected = [sens_text[i] for i in topk[idx]]\n",
    "        res = checker(sens_text_selected, sen)\n",
    "        if res:\n",
    "            numerator += 1\n",
    "        denominator += 1\n",
    "    return numerator / denominator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T02:07:05.503264Z",
     "start_time": "2023-11-02T02:07:05.492829Z"
    }
   },
   "id": "c33101f07b1a506d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'true'\n",
    "b = 'True'\n",
    "\n",
    "eval(b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:16:26.348433Z",
     "start_time": "2023-11-16T15:16:26.343586Z"
    }
   },
   "id": "84f08abbe284dcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
