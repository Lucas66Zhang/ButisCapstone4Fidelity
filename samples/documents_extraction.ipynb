{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(year_start:int, year_end:int) -> list[str]:\n",
    "    \"\"\"Extract links to the specific SEC case from directory based on year\n",
    "\n",
    "    Args:\n",
    "        year_start (_type_): start year of extracted document\n",
    "        year_end (_type_): end year of extracted document\n",
    "\n",
    "    Returns:\n",
    "        _type_: list of links for SEC documents\n",
    "    \"\"\"\n",
    "    site_list_cleaned = []\n",
    "    for year in tqdm(range(year_start, year_end)):\n",
    "        page = 0\n",
    "        while True:\n",
    "            links = f'https://www.sec.gov/litigation/litreleases?aId=edit-year&year={year}&page={page}'\n",
    "            response = requests.get(links)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            site_list = list(set(soup.find_all(\"tr\", {\"class\": \"pr-list-page-row\"})))\n",
    "            if len(site_list) == 0 :\n",
    "                break\n",
    "\n",
    "            for site in site_list:\n",
    "                if len(site.find_all(\"a\", {\"type\": \"application/pdf\"}, href=True)) == 1: #With only one pdf file\n",
    "                    site_list_cleaned.append('https://www.sec.gov/' + site.find_all(\"a\", href=True)[0].get('href'))\n",
    "            page += 1\n",
    "        \n",
    "    return site_list_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(site_lists:list[str])->pd.DataFrame():\n",
    "    \"\"\"Extract summary and pdf links from the website\n",
    "\n",
    "    Args:\n",
    "        site_lists (list[str]): _description_\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: First Columns contains links of SEC file documents\n",
    "                   Second Columns contains extracted summary of SEC file documents \n",
    "    \"\"\"\n",
    "    pdf_summary_df = pd.DataFrame(columns=['pdf_link', 'summary'])\n",
    "    for site_list in tqdm(site_lists):\n",
    "        response_site = requests.get(site_list)\n",
    "        soup_site = BeautifulSoup(response_site.text, 'html.parser')\n",
    "        paragraph_list = soup_site.find_all(\"p\", attrs = {'class' :None})\n",
    "        paragraph_combine = ''.join([str(paragraph) for paragraph in paragraph_list])\n",
    "        if len(soup_site.find_all(\"a\", href=re.compile(\"complaints\"))) != 0:\n",
    "            pdf_link = 'https://www.sec.gov/' + soup_site.find_all(\"a\", href=re.compile(\"complaints\"))[-1].get('href')\n",
    "            pdf_summary_df.loc[len(pdf_summary_df.index)] = [pdf_link, paragraph_combine]\n",
    "    return pdf_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply OCR to extract text from pdf link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pytesseract as tess  \n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "tess.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_name, txt_folder):\n",
    "    \"\"\"Extract text from pdf using OCR\n",
    "\n",
    "    Args:\n",
    "        file_name (_type_): input document path\n",
    "        txt_folder (_type_): output text file target path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Store all pages of one file here:\n",
    "    pages = []\n",
    "    output_file_name = txt_folder + '/' + file_name.split('/')[-1] + \".txt\"  # Generating output file name\n",
    "    if not os.path.isfile(output_file_name):\n",
    "      try:\n",
    "          # Convert the PDF file to a list of PIL images:\n",
    "          images = convert_from_path(file_name)  \n",
    "\n",
    "          # Extract text from each image:\n",
    "          for i, image in enumerate(images):\n",
    "            # Generating filename for each image\n",
    "              filename = \"page_\" + str(i) + \"_\" + os.path.basename(file_name) + \".jpeg\"  \n",
    "              image.save(filename, \"JPEG\")  \n",
    "            # Saving each image as JPEG\n",
    "              text = tess.image_to_string(Image.open(filename))  # Extracting text from each image using pytesseract\n",
    "              os.remove(filename)\n",
    "              pages.append(text)\n",
    "            # Appending extracted text to pages list\n",
    "\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "\n",
    "      # Write the extracted text to a file:\n",
    "      with open(output_file_name, \"w\") as f:\n",
    "          f.write(\"\\n\".join(pages))  \n",
    "        # Writing extracted text to output file\n",
    "\n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cropped_pdf(file_name, txt_folder):\n",
    "    \"\"\"Extract text from pdf using OCR with cropped pictures\n",
    "\n",
    "    Args:\n",
    "        file_name (_type_): input document path\n",
    "        txt_folder (_type_): output text file target path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Store all pages of one file here:\n",
    "    pages = []\n",
    "    output_file_name = txt_folder + '/' + file_name.split('/')[-1] + \".txt\"  # Generating output file name\n",
    "    if not os.path.isfile(output_file_name):\n",
    "      try:\n",
    "          # Convert the PDF file to a list of PIL images:\n",
    "          images = convert_from_path(file_name)  \n",
    "\n",
    "          # Extract text from each image:\n",
    "          for i, image in enumerate(images):\n",
    "              width, height = image.size\n",
    "              left = 180\n",
    "              top = 130\n",
    "              right = width - 100\n",
    "              bottom = height - 230\n",
    "              im1 = image.crop((left, top, right, bottom))\n",
    "            # Generating filename for each image\n",
    "              filename = \"page_\" + str(i) + \"_\" + os.path.basename(file_name) + \".jpeg\"  \n",
    "              im1.save(filename, \"JPEG\")  \n",
    "            # Saving each image as JPEG\n",
    "              text = tess.image_to_string(Image.open(filename))  # Extracting text from each image using pytesseract\n",
    "              os.remove(filename)\n",
    "              pages.append(text)\n",
    "            # Appending extracted text to pages list\n",
    "\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "\n",
    "      # Write the extracted text to a file:\n",
    "      with open(output_file_name, \"w\") as f:\n",
    "          f.write(\"\\n\".join(pages))  \n",
    "        # Writing extracted text to output file\n",
    "\n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Demo on OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p align=\"right\"&gt;CORRECTED&lt;/p&gt;&lt;p&gt;The Securitie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The United States Securities and Exchange C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission (\"Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission (\"Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_link  \\\n",
       "0  https://www.sec.gov//litigation/complaints/200...   \n",
       "1  https://www.sec.gov//litigation/complaints/200...   \n",
       "2  https://www.sec.gov//litigation/complaints/200...   \n",
       "3  https://www.sec.gov//litigation/complaints/200...   \n",
       "4  https://www.sec.gov//litigation/complaints/200...   \n",
       "\n",
       "                                             summary  \n",
       "0  <p align=\"right\">CORRECTED</p><p>The Securitie...  \n",
       "1  <p>The United States Securities and Exchange C...  \n",
       "2  <p>The Securities and Exchange Commission anno...  \n",
       "3  <p>The Securities and Exchange Commission (\"Co...  \n",
       "4  <p>The Securities and Exchange Commission (\"Co...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_summary_df = pd.read_csv('final_version_withouttext.csv', index_col=[0])\n",
    "pdf_summary_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropped Image first 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>text_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p align=\"right\"&gt;CORRECTED&lt;/p&gt;&lt;p&gt;The Securitie...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The United States Securities and Exchange C...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission anno...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission (\"Co...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/200...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission (\"Co...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/201...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission toda...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/201...</td>\n",
       "      <td>&lt;p&gt;On November 14, the Securities and Exchange...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/201...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission toda...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/201...</td>\n",
       "      <td>&lt;p&gt;On October 10, 2013, the Securities and Exc...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>https://www.sec.gov//litigation/complaints/201...</td>\n",
       "      <td>&lt;p&gt;The Securities and Exchange Commission toda...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pdf_link  \\\n",
       "0    https://www.sec.gov//litigation/complaints/200...   \n",
       "1    https://www.sec.gov//litigation/complaints/200...   \n",
       "2    https://www.sec.gov//litigation/complaints/200...   \n",
       "3    https://www.sec.gov//litigation/complaints/200...   \n",
       "4    https://www.sec.gov//litigation/complaints/200...   \n",
       "..                                                 ...   \n",
       "995  https://www.sec.gov//litigation/complaints/201...   \n",
       "996  https://www.sec.gov//litigation/complaints/201...   \n",
       "997  https://www.sec.gov//litigation/complaints/201...   \n",
       "998  https://www.sec.gov//litigation/complaints/201...   \n",
       "999  https://www.sec.gov//litigation/complaints/201...   \n",
       "\n",
       "                                               summary text_extracted  \n",
       "0    <p align=\"right\">CORRECTED</p><p>The Securitie...                 \n",
       "1    <p>The United States Securities and Exchange C...                 \n",
       "2    <p>The Securities and Exchange Commission anno...                 \n",
       "3    <p>The Securities and Exchange Commission (\"Co...                 \n",
       "4    <p>The Securities and Exchange Commission (\"Co...                 \n",
       "..                                                 ...            ...  \n",
       "995  <p>The Securities and Exchange Commission toda...                 \n",
       "996  <p>On November 14, the Securities and Exchange...                 \n",
       "997  <p>The Securities and Exchange Commission toda...                 \n",
       "998  <p>On October 10, 2013, the Securities and Exc...                 \n",
       "999  <p>The Securities and Exchange Commission toda...                 \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_summary_df_cropped_100 = pd.read_csv('final_version_withtext.csv', index_col=[0])\n",
    "pdf_summary_df_cropped_100 = pdf_summary_df_cropped_100.iloc[:1000,:]\n",
    "pdf_summary_df_cropped_100 = pdf_summary_df_cropped_100.reset_index(drop=True)\n",
    "pdf_summary_df_cropped_100['text_extracted'] = ''\n",
    "pdf_summary_df_cropped_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "988it [2:42:38, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error (2): Illegal character <21> in hex string\n",
      "Syntax Error (4): Illegal character <4f> in hex string\n",
      "Syntax Error (6): Illegal character <54> in hex string\n",
      "Syntax Error (7): Illegal character <59> in hex string\n",
      "Syntax Error (8): Illegal character <50> in hex string\n",
      "Syntax Error (11): Illegal character <68> in hex string\n",
      "Syntax Error (12): Illegal character <74> in hex string\n",
      "Syntax Error (13): Illegal character <6d> in hex string\n",
      "Syntax Error (14): Illegal character <6c> in hex string\n",
      "Syntax Error (18): Illegal character <68> in hex string\n",
      "Syntax Error (19): Illegal character <74> in hex string\n",
      "Syntax Error (20): Illegal character <6d> in hex string\n",
      "Syntax Error (21): Illegal character <6c> in hex string\n",
      "Syntax Error (23): Illegal character <6c> in hex string\n",
      "Syntax Error (25): Illegal character <6e> in hex string\n",
      "Syntax Error (26): Illegal character <67> in hex string\n",
      "Syntax Error (27): Illegal character <3d> in hex string\n",
      "Syntax Error (28): Illegal character <22> in hex string\n",
      "Syntax Error (30): Illegal character <6e> in hex string\n",
      "Syntax Error (31): Illegal character <22> in hex string\n",
      "Syntax Error (34): Illegal character <69> in hex string\n",
      "Syntax Error (35): Illegal character <72> in hex string\n",
      "Syntax Error (36): Illegal character <3d> in hex string\n",
      "Syntax Error (37): Illegal character <22> in hex string\n",
      "Syntax Error (38): Illegal character <6c> in hex string\n",
      "Syntax Error (39): Illegal character <74> in hex string\n",
      "Syntax Error (40): Illegal character <72> in hex string\n",
      "Syntax Error (41): Illegal character <22> in hex string\n",
      "Syntax Error (47): Illegal character <68> in hex string\n",
      "Syntax Error (58): Illegal character <6d> in hex string\n",
      "Syntax Error (60): Illegal character <74> in hex string\n",
      "Syntax Error (64): Illegal character <68> in hex string\n",
      "Syntax Error (66): Illegal character <72> in hex string\n",
      "Syntax Error (67): Illegal character <73> in hex string\n",
      "Syntax Error (69): Illegal character <74> in hex string\n",
      "Syntax Error (70): Illegal character <3d> in hex string\n",
      "Syntax Error (71): Illegal character <22> in hex string\n",
      "Syntax Error (72): Illegal character <75> in hex string\n",
      "Syntax Error (73): Illegal character <74> in hex string\n",
      "Syntax Error (75): Illegal character <2d> in hex string\n",
      "Syntax Error (77): Illegal character <22> in hex string\n",
      "Syntax Error (79): Illegal character <2f> in hex string\n",
      "Syntax Error (82): Illegal character <73> in hex string\n",
      "Syntax Error (84): Illegal character <72> in hex string\n",
      "Syntax Error (85): Illegal character <69> in hex string\n",
      "Syntax Error (86): Illegal character <70> in hex string\n",
      "Syntax Error (87): Illegal character <74> in hex string\n",
      "Syntax Error (89): Illegal character <74> in hex string\n",
      "Syntax Error (90): Illegal character <79> in hex string\n",
      "Syntax Error (91): Illegal character <70> in hex string\n",
      "Syntax Error (93): Illegal character <3d> in hex string\n",
      "Syntax Error (94): Illegal character <22> in hex string\n",
      "Syntax Error (95): Illegal character <74> in hex string\n",
      "Syntax Error (97): Illegal character <78> in hex string\n",
      "Syntax Error (98): Illegal character <74> in hex string\n",
      "Syntax Error (99): Illegal character <2f> in hex string\n",
      "Syntax Error (100): Illegal character <6a> in hex string\n",
      "Syntax Error (102): Illegal character <76> in hex string\n",
      "Syntax Error (104): Illegal character <73> in hex string\n",
      "Syntax Error (106): Illegal character <72> in hex string\n",
      "Syntax Error (107): Illegal character <69> in hex string\n",
      "Syntax Error (108): Illegal character <70> in hex string\n",
      "Syntax Error (109): Illegal character <74> in hex string\n",
      "Syntax Error (110): Illegal character <22> in hex string\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [2:47:01, 10.02s/it]\n"
     ]
    }
   ],
   "source": [
    "tess.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "for index, row in tqdm(pdf_summary_df_cropped_100.iterrows()):\n",
    "    \n",
    "    pdf_path = os.path.join(\"Documents_pdf_cropped\", (row['pdf_link'].split('/')[-1]))\n",
    "    pdf_path = pdf_path.replace('\\\\', '/')\n",
    "    try:\n",
    "        response_pdf = requests.get(row['pdf_link'])\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            f.write(response_pdf.content)\n",
    "\n",
    "        read_cropped_pdf(pdf_path, 'Documents_txt_cropped')\n",
    "        \n",
    "        row['text_extracted'] = open('./Documents_txt_cropped/'+pdf_path.split('/')[-1] +'.txt', 'r').read()\n",
    "        pdf_summary_df_cropped_100.iloc[index, -1] =  row['text_extracted']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_break(sample:str) ->str:\n",
    "    \"\"\"Delete extra line breaks\n",
    "\n",
    "    Args:\n",
    "        sample (str): original text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    pattern = r'(?<!\\n)\\n(?!\\\\n)'\n",
    "    sample = re.sub(pattern, ' ', sample)\n",
    "    return sample\n",
    "\n",
    "def pages(sample:str) ->str:\n",
    "    \"\"\"Delete the page number\n",
    "\n",
    "    Args:\n",
    "        sample (str): original text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    pattern = r'\\nCase\\s*\\d+:\\s*\\d+-cv-\\d+\\s*Document\\s*\\d+\\s*Filed\\s*\\d+/\\d+/\\d+\\s*Page\\s*\\d+\\s*of\\s*\\d+\\s*\\n'\n",
    "    sample = re.sub(pattern, ' ', sample)\n",
    "    sample = sample.replace('|', '')\n",
    "    return sample\n",
    "\n",
    "def html_to_text(sample):\n",
    "    \"\"\"Delete the HTML symbols\n",
    "    Args:\n",
    "        sample (str): original text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(sample, 'html.parser')\n",
    "    plain_text = soup.get_text()\n",
    "    return plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_summary_df_cropped_100 = pd.read_csv('../data/final_version_cropped_first1000.csv')\n",
    "pdf_summary_df_cropped_100['text_extracted'] = [line_break(str(ele)) for ele in pdf_summary_df_cropped_100['text_extracted']]\n",
    "pdf_summary_df_cropped_100['text_extracted'] = [pages(ele) for ele in pdf_summary_df_cropped_100['text_extracted']]\n",
    "pdf_summary_df_cropped_100['text_extracted'] = [html_to_text(ele) for ele in pdf_summary_df_cropped_100['text_extracted']]\n",
    "pdf_summary_df_cropped_100['summary'] = [line_break(str(ele)) for ele in pdf_summary_df_cropped_100['summary']]\n",
    "pdf_summary_df_cropped_100['summary'] = [pages(ele) for ele in pdf_summary_df_cropped_100['summary']]\n",
    "pdf_summary_df_cropped_100['summary'] = [html_to_text(ele) for ele in pdf_summary_df_cropped_100['summary']]\n",
    "\n",
    "pdf_summary_df_cropped_100.to_csv('../data/final_version_cropped_first1000.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
