{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Presentaion of Our Methods\n",
    "The topic of our project is to design a metric to evaluate teh quality of summaries given the original text. After discussion and paper reading, we think that a good metric for this task should have the following properties:\n",
    "1. It should be able to tell a good summary from a bad one (The scores of them should be as different as possible).\n",
    "2. It should be able to discern varying degrees of factual distortion (Given any two summaries according to the same document, the worse one should be scored lower).\n",
    "3. It should be able to make evaluation based on the detail of the summary (Give the reason why it makes such an evaluation result).\n",
    "\n",
    "In this notebook, we will present our methods for the project and the results we obtained. The presentation will be divided into following parts, each part will cover the above three properties:\n",
    "1. Results of baseline metrics\n",
    "2. Results of our methods\n",
    "3. Comparison of our methods with baseline metrics\n",
    "4. Conclusion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b6b9356b2e6b53"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T03:54:52.157663Z",
     "start_time": "2023-11-28T03:54:46.563510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from pipeline import SummaryGrader, NER_comparison, highlight"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Results of baseline metrics\n",
    "TO DO, (introdece the metrics we used as baseline, and the results we obtained.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f8e4008bafb9d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO DO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58a3d53f7019c97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Results of our methods\n",
    "We designed two methods to evaluate the quality of summaries.\n",
    "1. Named entity comparison: we compare the named entities in the summary with the named entities in the original text by `NER_comparison`. It will calculate two ratios, one is the ratio of named entities in the summary that are also in the original text, the other is the ratio of named entities in the original text that are also in the summary. In addition, It also provides a method `.comparison_display()` to highlight the named entities in the summary that are not in the original text, or the named entities in the original text that are not in the summary. This will help users to find the details of the result.\n",
    "2. Summary grading based on sentence-level checking: we apply LLMs to help us check the consistency between the summary and the original text sentence by sentence through `SummaryGrader`. Its `.process()` method can return the ratio of sentences in the summary that are thought to be consistent with the original text and the list of indices of sentences in the summary that are thought to be inconsistent with the original text. In addition, we can use `highlight()` function to highlight the sentences in the summary that are thought to be inconsistent with the original text. This will help users to find the details of the result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ecc914f5a125af6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Named entity comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02f944b16991fcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO DO just show how to use the class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53617e3c0c69c084"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Summary grading based on sentence-level checking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1212c6b23c5dc564"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO DO show the result of the class from three properties above"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "681aa4d30f46033e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Comparison of our methods with baseline metrics\n",
    "In this section, we will compare our methods with the baseline metrics we used in the first section from the three properties we mentioned at the beginning. We will use the same dataset and the same summaries as in the first section. The results are as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9dd3307ac90eb86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO DO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f32033aee838b53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "TO DO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f382f0a87f894221"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
